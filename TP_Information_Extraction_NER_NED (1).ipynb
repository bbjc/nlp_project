{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrcpNmrnYXAC"
   },
   "source": [
    "# **TLFT Lab: Named Entity Recognition and Linking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f-qzWizbxO_"
   },
   "source": [
    "**For training, do not forget to make sure that the GPU is activated!**\n",
    "\n",
    "Exécution -> Modifier le type d'exécution -> GPU T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6BCMci9T0yi"
   },
   "source": [
    "# Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:18:08.000045Z",
     "iopub.status.busy": "2025-03-05T13:18:07.999468Z",
     "iopub.status.idle": "2025-03-05T13:18:36.005988Z",
     "shell.execute_reply": "2025-03-05T13:18:36.004896Z",
     "shell.execute_reply.started": "2025-03-05T13:18:08.000006Z"
    },
    "id": "vksSwpHOo6h-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/lib/python3/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (15.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=df4f0f3d33e425aad5b092bab7cfc68ebd3c80a1dc50d9b9d6f5a9961c2392d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "#!pip install tensorflow\n",
    "#!pip install evaluate\n",
    "#!pip install transformers\n",
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH2asAcIT5Hi"
   },
   "source": [
    "# Retrieve necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:19:17.377126Z",
     "iopub.status.busy": "2025-03-05T13:19:17.376729Z",
     "iopub.status.idle": "2025-03-05T13:19:52.070740Z",
     "shell.execute_reply": "2025-03-05T13:19:52.069252Z",
     "shell.execute_reply.started": "2025-03-05T13:19:17.377090Z"
    },
    "id": "SArmszM3ETM8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-05 13:19:17--  https://github.com/gbella/NLP/raw/refs/heads/main/NER/ner.zip\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/gbella/NLP/refs/heads/main/NER/ner.zip [following]\n",
      "--2025-03-05 13:19:17--  https://raw.githubusercontent.com/gbella/NLP/refs/heads/main/NER/ner.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2963108 (2.8M) [application/zip]\n",
      "Saving to: ‘ner.zip’\n",
      "\n",
      "ner.zip             100%[===================>]   2.83M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-03-05 13:19:18 (107 MB/s) - ‘ner.zip’ saved [2963108/2963108]\n",
      "\n",
      "--2025-03-05 13:19:18--  https://github.com/gbella/NLP/raw/refs/heads/main/NER/run_ner.py\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/gbella/NLP/refs/heads/main/NER/run_ner.py [following]\n",
      "--2025-03-05 13:19:18--  https://raw.githubusercontent.com/gbella/NLP/refs/heads/main/NER/run_ner.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27813 (27K) [text/plain]\n",
      "Saving to: ‘run_ner.py’\n",
      "\n",
      "run_ner.py          100%[===================>]  27.16K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-05 13:19:18 (97.3 MB/s) - ‘run_ner.py’ saved [27813/27813]\n",
      "\n",
      "Archive:  ner.zip\n",
      "  inflating: ner_test.json           \n",
      "  inflating: ner_train.json          \n",
      "  inflating: ner_valid.json          \n",
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 260146, done.\u001b[K\n",
      "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 260146 (delta 2), reused 0 (delta 0), pack-reused 260132 (from 1)\u001b[K\n",
      "Receiving objects: 100% (260146/260146), 269.80 MiB | 25.70 MiB/s, done.\n",
      "Resolving deltas: 100% (192167/192167), done.\n",
      "Updating files: 100% (5072/5072), done.\n"
     ]
    }
   ],
   "source": [
    "#!wget https://github.com/gbella/NLP/raw/refs/heads/main/NER/ner.zip\n",
    "#!wget https://github.com/gbella/NLP/raw/refs/heads/main/NER/run_ner.py\n",
    "#!unzip ner.zip\n",
    "#!mkdir output\n",
    "#!rm -rf transformers/\n",
    "#!git clone https://github.com/huggingface/transformers.git\n",
    "#!cp run_ner.py transformers/examples/tensorflow/token-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmWXgJrtT89w"
   },
   "source": [
    "# 1. Fine-tune a BERT model for named entity recognition\n",
    "\n",
    "BERT is a generic language model based on a Transformer encoder. In order to use it for NER, we need to fine-tune it. Below we use an existing Hugging Face script to fine-tune the **bert-base-cased** model for English NER. We need the **cased** model as initial upper/lowercase is an important feature for names in English. We use the small **base** model for fast training: on our training data, one epoch of fine-tuning takes about 10 minutes using the Google Colab GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:21:03.515314Z",
     "iopub.status.busy": "2025-03-05T13:21:03.514716Z",
     "iopub.status.idle": "2025-03-05T13:32:43.435826Z",
     "shell.execute_reply": "2025-03-05T13:32:43.435095Z",
     "shell.execute_reply.started": "2025-03-05T13:21:03.515283Z"
    },
    "id": "MWcuqBfYm-w-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 13:21:07.479552: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-05 13:21:07.479686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-05 13:21:07.481874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 13:21:07.494006: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-05 13:21:09.033740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading data files: 100%|███████████████████| 2/2 [00:00<00:00, 9383.23it/s]\n",
      "Extracting data files: 100%|██████████████████████| 2/2 [00:00<00:00, 20.76it/s]\n",
      "Generating train split: 30000 examples [00:00, 289155.32 examples/s]\n",
      "Generating validation split: 9013 examples [00:00, 313566.49 examples/s]\n",
      "config.json: 100%|█████████████████████████████| 570/570 [00:00<00:00, 1.55MB/s]\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-art\",\n",
      "    \"1\": \"B-eve\",\n",
      "    \"2\": \"B-geo\",\n",
      "    \"3\": \"B-gpe\",\n",
      "    \"4\": \"B-nat\",\n",
      "    \"5\": \"B-org\",\n",
      "    \"6\": \"B-per\",\n",
      "    \"7\": \"B-tim\",\n",
      "    \"8\": \"I-art\",\n",
      "    \"9\": \"I-eve\",\n",
      "    \"10\": \"I-geo\",\n",
      "    \"11\": \"I-gpe\",\n",
      "    \"12\": \"I-nat\",\n",
      "    \"13\": \"I-org\",\n",
      "    \"14\": \"I-per\",\n",
      "    \"15\": \"I-tim\",\n",
      "    \"16\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-art\": 0,\n",
      "    \"B-eve\": 1,\n",
      "    \"B-geo\": 2,\n",
      "    \"B-gpe\": 3,\n",
      "    \"B-nat\": 4,\n",
      "    \"B-org\": 5,\n",
      "    \"B-per\": 6,\n",
      "    \"B-tim\": 7,\n",
      "    \"I-art\": 8,\n",
      "    \"I-eve\": 9,\n",
      "    \"I-geo\": 10,\n",
      "    \"I-gpe\": 11,\n",
      "    \"I-nat\": 12,\n",
      "    \"I-org\": 13,\n",
      "    \"I-per\": 14,\n",
      "    \"I-tim\": 15,\n",
      "    \"O\": 16\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|██████████████████| 49.0/49.0 [00:00<00:00, 218kB/s]\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "vocab.txt: 100%|█████████████████████████████| 213k/213k [00:00<00:00, 24.4MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 436k/436k [00:00<00:00, 41.4MB/s]\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset: 100%|█| 30000/30000 [00:04<00:00, 7347.86 examples\n",
      "Running tokenizer on dataset: 100%|█| 9013/9013 [00:01<00:00, 5056.62 examples/s\n",
      "Sample 20952 of the training set: {'labels': [-100, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 14, 14, 14, 16, -100], 'input_ids': [101, 5231, 112, 188, 2699, 1555, 1144, 5762, 1251, 6083, 1107, 1103, 17539, 1104, 4078, 5200, 3234, 13845, 10684, 2737, 18599, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
      "Sample 3648 of the training set: {'labels': [-100, 16, 16, 16, 16, 3, 16, 16, 16, 16, 2, 2, 2, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 16, -100], 'input_ids': [101, 15993, 1142, 1989, 117, 9651, 3878, 1163, 170, 6321, 158, 119, 156, 119, 22020, 2035, 1841, 1120, 1655, 1210, 1234, 1107, 10727, 3658, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
      "Sample 819 of the training set: {'labels': [-100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 3, 16, 16, 7, 16, 5, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100], 'input_ids': [101, 1130, 1330, 1718, 117, 2021, 1474, 170, 6456, 118, 4013, 4744, 3269, 5985, 9626, 1194, 1103, 1334, 1104, 1126, 13099, 2306, 3592, 9170, 1107, 23321, 117, 3646, 1103, 3445, 1105, 5785, 1158, 1853, 1234, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
      "Tensorflow: setting up strategy\n",
      "2025-03-05 13:21:19.528904: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:19.612705: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:19.613357: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:19.617802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:19.618262: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:19.618596: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:25.835359: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:25.835646: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:25.835896: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:21:25.836054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15108 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:00:05.0, compute capability: 6.1\n",
      "model.safetensors: 100%|██████████████████████| 436M/436M [00:02<00:00, 216MB/s]\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors\n",
      "Loaded 107,719,680 parameters in the TF 2.0 model.\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss. You can also specify `loss='auto'` to get the internal loss without printing this info string.\n",
      "Downloading builder script: 100%|██████████| 6.34k/6.34k [00:00<00:00, 15.5MB/s]\n",
      "***** Running training *****\n",
      "  Num examples = 30000\n",
      "  Num Epochs = 1.0\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size = 8\n",
      "2025-03-05 13:22:16.729994: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f56842a9720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-03-05 13:22:16.730065: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P5000, Compute Capability 6.1\n",
      "2025-03-05 13:22:16.772451: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-05 13:22:17.704612: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741180937.883321     173 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "3750/3750 [==============================] - 580s 138ms/step - loss: 0.1588 - val_loss: 0.1298\n",
      "1127/1127 [==============================] - 57s 46ms/step\n",
      "Evaluation metrics:\n",
      "precision: 0.8221\n",
      "recall: 0.8113\n",
      "f1: 0.8167\n",
      "accuracy: 0.9584\n",
      "Configuration saved in output/config.json\n",
      "Model weights saved in output/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/tensorflow/token-classification/run_ner.py \\\n",
    "  --model_name_or_path bert-base-cased \\\n",
    "  --train_file ner_train.json \\\n",
    "  --validation_file ner_valid.json \\\n",
    "  --test_file ner_test.json \\\n",
    "  --text_column_name tokens \\\n",
    "  --label_column_name labels \\\n",
    "  --output_dir output \\\n",
    "  --label_all_tokens \\\n",
    "  --do_train \\\n",
    "  --num_train_epochs 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWTnQKDxS9Mf"
   },
   "outputs": [],
   "source": [
    "# In case training does not work, you can retrieve a trained model from here:\n",
    "#!wget https://github.com/gbella/NLP/raw/refs/heads/main/NER/NER_finetuned_bert_base.zip\n",
    "#!mkdir output\n",
    "#!unzip NER_finetuned_bert_base.zip -d output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXRH44aVU2m0"
   },
   "source": [
    "# Try out the finetuned model\n",
    "Retrieve a paragraph of text from an English news article that contains several names of well-known entities (cities, countries, famous people, dates, etc.). You can, for example, find appropriate text on http://www.theguardian.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:37:58.940118Z",
     "iopub.status.busy": "2025-03-05T13:37:58.939225Z",
     "iopub.status.idle": "2025-03-05T13:38:19.277142Z",
     "shell.execute_reply": "2025-03-05T13:38:19.276433Z",
     "shell.execute_reply.started": "2025-03-05T13:37:58.940080Z"
    },
    "id": "NQvkb5JRMdRG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 13:38:01.711090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-05 13:38:01.711172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-05 13:38:01.712771: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 13:38:01.723308: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-05 13:38:03.113276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-03-05 13:38:07.264050: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:07.271988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:07.272241: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:07.273670: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:07.273841: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:07.273985: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:13.561890: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:13.562163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:13.562335: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-03-05 13:38:13.562467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15108 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:00:05.0, compute capability: 6.1\n",
      "All TF 2.0 model weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-per', 'score': 0.9934242, 'index': 4, 'word': 'President', 'start': 4, 'end': 13}\n",
      "{'entity': 'I-per', 'score': 0.99419785, 'index': 5, 'word': 'Donald', 'start': 14, 'end': 20}\n",
      "{'entity': 'I-per', 'score': 0.995488, 'index': 6, 'word': 'Trump', 'start': 21, 'end': 26}\n",
      "{'entity': 'B-geo', 'score': 0.96637505, 'index': 12, 'word': 'Ukraine', 'start': 58, 'end': 65}\n",
      "{'entity': 'B-org', 'score': 0.9936946, 'index': 26, 'word': 'White', 'start': 138, 'end': 143}\n",
      "{'entity': 'I-org', 'score': 0.9893708, 'index': 27, 'word': 'House', 'start': 144, 'end': 149}\n",
      "{'entity': 'B-per', 'score': 0.98461413, 'index': 31, 'word': 'Mike', 'start': 176, 'end': 180}\n",
      "{'entity': 'I-per', 'score': 0.9902683, 'index': 32, 'word': 'Waltz', 'start': 181, 'end': 186}\n",
      "{'entity': 'B-tim', 'score': 0.99830854, 'index': 35, 'word': 'Wednesday', 'start': 195, 'end': 204}\n",
      "{'entity': 'B-org', 'score': 0.97797513, 'index': 37, 'word': 'Re', 'start': 206, 'end': 208}\n",
      "{'entity': 'B-org', 'score': 0.98367256, 'index': 38, 'word': '##uters', 'start': 208, 'end': 213}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import BertConfig, BertForTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"bert-base-cased\",\n",
    "            use_fast=True,\n",
    "        )\n",
    "my_model = BertForTokenClassification.from_pretrained(\"./output/\", from_tf=True)\n",
    "nlp = pipeline('ner', model=my_model, tokenizer=my_tokenizer)\n",
    "text = \"*** President Donald Trump will consider restoring aid to Ukraine if peace talks are arranged and confidence-building measures are taken, White House national security adviser Mike Waltz said on Wednesday, Reuters reported. ***\"\n",
    "result = nlp(text)\n",
    "for token in result:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fwg_6_jCS4LL"
   },
   "source": [
    "# 2. Reconstituting named entities from subwords\n",
    "\n",
    "As you can also observe from the output above, the Transformer's tokeniser produces subword tokens, to which named entity tags are attached. For further processing, we need to reconstitute entire named entities. The goal of this exercise is to collect all named entities as a list or as a set of (name, tag) pairs:\n",
    "\n",
    "{(\"New York\", \"gpe\"), (\"Jacques Chirac\", \"per\"), (\"NATO\", \"org\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T13:49:27.202034Z",
     "iopub.status.busy": "2025-03-05T13:49:27.201233Z",
     "iopub.status.idle": "2025-03-05T13:49:27.209027Z",
     "shell.execute_reply": "2025-03-05T13:49:27.208051Z",
     "shell.execute_reply.started": "2025-03-05T13:49:27.202005Z"
    },
    "id": "JN4HWYlYSVEz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('President Donald Trump', 'per'), ('Wednesday', 'tim'), ('Mike Waltz', 'per'), ('Reuters', 'org'), ('Ukraine', 'geo'), ('White House', 'org')}\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "entities = []\n",
    "for entry in result:\n",
    "  token = entry[\"word\"]\n",
    "  entity = entry[\"entity\"]\n",
    "  if token.startswith(\"##\"):\n",
    "    words[-1] = words[-1] + token[2:]\n",
    "  else:\n",
    "    if entity.startswith(\"I\"):\n",
    "      words[-1] = words[-1] + \" \" + token\n",
    "    else:\n",
    "      words.append(token)\n",
    "      entities.append(entity[2:])\n",
    "# by using a set, we remove duplicate entries\n",
    "word_entity_pairs = set(zip(words, entities))\n",
    "print(word_entity_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXSQFYWISfu7"
   },
   "source": [
    "# 3. Entity Linking: Retrieval of Candidate Entities\n",
    "\n",
    "We link the entities found in the news article to entries in Wikidata, and retrieve further information about them. We use the *wbsearchentities* endpoint of the Wikidata API (https://www.wikidata.org/w/api.php) to search for entities by name. Note that multiple Wikidata entities can correspond to the same name (e.g. Paris is a city in France, a city in Texas, and also a person's name).\n",
    "\n",
    "Then, as a second step, we retrieve detailed information about each entity using the *wbgetentities* endpoint. We store all relevant information in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T14:36:06.849207Z",
     "iopub.status.busy": "2025-03-05T14:36:06.848782Z",
     "iopub.status.idle": "2025-03-05T14:36:17.538087Z",
     "shell.execute_reply": "2025-03-05T14:36:17.537354Z",
     "shell.execute_reply.started": "2025-03-05T14:36:06.849173Z"
    },
    "id": "JLC5BRYdvxda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikidata entities corresponding to 'President Donald Trump/per':\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q22686', 'text': 'President Donald Trump', 'description': 'president of the United States (2017–2021, 2025–present)', 'types': ['Q5']}\n",
      "Wikidata entities corresponding to 'Mike Waltz/per':\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q55386653', 'text': 'Mike Waltz', 'description': 'U.S. National Security Advisor since 2025', 'types': ['Q5']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q95813934', 'text': 'Mike Waltze', 'description': '', 'types': ['Q5']}\n",
      "Wikidata entities corresponding to 'Reuters/org':\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q130879', 'text': 'Reuters', 'description': 'international news agency', 'types': ['Q192283', 'Q4830453']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q83377360', 'text': 'Reuters', 'description': 'family name', 'types': ['Q101352']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q1509580', 'text': 'Reuters', 'description': 'human settlement in Germany', 'types': ['Q253019']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q112662578', 'text': 'Reuters-21578', 'description': 'dataset', 'types': ['Q1172284']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q50986', 'text': 'Reuterstadt Stavenhagen', 'description': 'place in Mecklenburg-Vorpommern, Germany', 'types': ['Q42744322']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q22343541', 'text': 'reuters.com', 'description': 'news website', 'types': ['Q1153191']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q62526396', 'text': 'Reutershagen tram stop', 'description': 'tram stop in Rostock', 'types': ['Q2175765', 'Q953806']}\n",
      "Wikidata entities corresponding to 'Ukraine/geo':\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q212', 'text': 'Ukraine', 'description': 'country in Eastern Europe', 'types': ['Q3624078', 'Q6256', 'Q179164', 'Q7270', 'Q619610', 'Q4209223', 'Q4835091']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q133356', 'text': 'Ukraine', 'description': 'sovereign state (1919–1922) and a republic of the Soviet Union (1922–1991); one of the founding members of the UN in 1945', 'types': ['Q3024240', 'Q179164', 'Q3624078', 'Q236036', 'Q3624078', 'Q23037160']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q243610', 'text': 'Ukraine', 'description': 'country in Eastern Europe (1917–1921); government in exile (1921–1992)', 'types': ['Q3024240', 'Q788176', 'Q3624078', 'Q678116', 'Q3624078', 'Q465613']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q110999040', 'text': 'Ukraine invasion', 'description': 'ongoing military conflict in Eastern Europe since 2022', 'types': ['Q198', 'Q645883', 'Q2001676', 'Q467011', 'Q7883019', 'Q11422542', 'Q474138', 'Q1827102']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q170403', 'text': \"Ukraine men's national football team\", 'description': \"men's national association football team representing Ukraine\", 'types': ['Q6979593']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q2296961', 'text': 'Ukraine Stadium', 'description': 'multi-purpose stadium in Lviv, Ukraine', 'types': ['Q1154710']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q4220180', 'text': 'Ukraine', 'description': 'ethographic and literary chronicle', 'types': ['Q11032']}\n",
      "Wikidata entities corresponding to 'White House/org':\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q7903', 'text': 'White House', 'description': 'largest city of Morocco', 'types': ['Q515']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q35525', 'text': 'White House', 'description': 'official residence and office of the President of the United States', 'types': ['Q1802963', 'Q52177058', 'Q570116', 'Q2114972']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q1355327', 'text': 'White House', 'description': 'U.S. federal government office of the president of the United States', 'types': ['Q327333', 'Q20857065']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q1956766', 'text': 'White House', 'description': 'official website of the White House', 'types': ['Q123155338']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q130556185', 'text': 'White House', 'description': 'Czech company', 'types': ['Q4830453', 'Q6881511']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q63908539', 'text': 'White House Telephone Recordings and Transcripts (NAID 187903)', 'description': \"series in the National Archives and Records Administration's holdings\", 'types': ['Q2668072']}\n",
      "\t{'uri': 'http://www.wikidata.org/entity/Q2787872', 'text': 'White House', 'description': 'city in Tennessee, United States of America', 'types': ['Q1093829']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "#-------------------------------------------------------------\n",
    "# Querying an entity from Wikidata\n",
    "#-------------------------------------------------------------\n",
    "def fetch_wikidata(params):\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    try:\n",
    "        return requests.get(url, params=params)\n",
    "    except:\n",
    "        return 'Error calling the Wikidata API.'\n",
    "\n",
    "\n",
    "params = {\n",
    "        'action':   'wbsearchentities',\n",
    "        'format':   'json',\n",
    "        'language': 'en',\n",
    "        'uselang':  'en' # set it to the language in which you would like to receive results\n",
    "    }\n",
    "\n",
    "WIKITYPES = {\"per\": [\"Q5\"], \"org\": [\"Q42\"]} # The Wikidata type equivalents for some of our entity labels\n",
    "SKIPPED_TYPES = [\"tim\"] # we will not look for such entities in Wikidata\n",
    "wiki_entities = {}\n",
    "\n",
    "for word, label in word_entity_pairs:\n",
    "  if label in SKIPPED_TYPES:\n",
    "    continue # do not look up Time or Date entities\n",
    "  params['search'] = word\n",
    "  data = fetch_wikidata(params).json()\n",
    "  dataDict = dict(data)\n",
    "\n",
    "  print(\"Wikidata entities corresponding to '\" + word + \"/\" + label + \"':\")\n",
    "\n",
    "  for result in dataDict[\"search\"]:\n",
    "    if \"description\" not in result:\n",
    "      description = \"\"\n",
    "    else:\n",
    "      description = result[\"description\"]\n",
    "    identifier = result[\"id\"]\n",
    "    parameters = {\n",
    "            'action': 'wbgetentities',\n",
    "            'format': 'json',\n",
    "            'ids': identifier,\n",
    "            'languages': 'en'\n",
    "        }\n",
    "\n",
    "    entityDetails = fetch_wikidata(parameters).json()\n",
    "    for key in entityDetails[\"entities\"]:\n",
    "      value = entityDetails[\"entities\"][key]\n",
    "      if \"P31\" not in value[\"claims\"]:\n",
    "        # it does not have an instance-of relation => it is not a named entity => skip it\n",
    "        continue\n",
    "      entity_types = []\n",
    "      for typ in value[\"claims\"][\"P31\"]:\n",
    "        entity_types.append(typ[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"])\n",
    "      if (word,label) not in wiki_entities:\n",
    "        wiki_entities[(word,label)] = []\n",
    "      entity = {\"uri\": result[\"concepturi\"], \"text\": result[\"match\"][\"text\"], \"description\": description, \"types\": entity_types}\n",
    "      wiki_entities[(word,label)].append(entity)\n",
    "      print(\"\\t\" + str(entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEvkOJnrOqAZ"
   },
   "source": [
    "# 4. Entity Linking: Disambiguation\n",
    "\n",
    "As multiple entities can correspond to the same name, the candidate entities retrieved in the previous step need to be disambiguated: the entity that is the most relevant to the given text needs to be chosen. This process is called *named entity disambiguation* or *reconciliation*.\n",
    "\n",
    "As a first filtering step, we can use the named entity label (per, loc, gpe, org, tim, etc.) to constrain Wikidata results. For example, Paris may mean the city in France or a character from the Iliad. If our named entity annotation is \"per\", then the city entity can be filtered out and the character entity kept.\n",
    "\n",
    "The *wbgetentities* endpoint of the Wikidata API returned *instance-of* types for its entries: the code for the instance-of property is \"P31\". The types returned are, however, much more fine-grained than our named entity labels, so it is not trivial to map the two together. In this simple exercise, for the sake of example, we manually map a few labels to Wikidata types (e.g. \"person\" is mapped to the type \"Q5\").\n",
    "\n",
    "In a second disambiguation step, we will use a crude method, which is nevertheless efficient: we select the \"most common\" of the entries, i.e. the meaning that is the most frequently used. For example, on the whole, the name \"Paris\" refers much more frequently to the city in France than to the city in Texas (unless you live in Texas). While Wikidata does not provide such frequency data, we will approximate it by choosing the entry that has the lowest Wikidata ID. In disambiguation, such context-independent, frequency-based methods are widely used as strong baselines, as typically they yield an accuracy well above 50% while being extremely simple.\n",
    "\n",
    "**Perspective:** in order to improve this baseline disambiguation method, one can combine it with methods that take context into account. For example, one could compare Wikidata entity descriptions to the input text and choose the entity with the most similar description. The similarity could be computed using, for example:\n",
    " - a simple Jaccard similarity measure based on the proportion of shared words;\n",
    " - using cosine similarity over word or document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T14:43:52.805526Z",
     "iopub.status.busy": "2025-03-05T14:43:52.804853Z",
     "iopub.status.idle": "2025-03-05T14:43:52.813257Z",
     "shell.execute_reply": "2025-03-05T14:43:52.812255Z",
     "shell.execute_reply.started": "2025-03-05T14:43:52.805498Z"
    },
    "id": "BR05BufWIPI2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disambiguated entities:\n",
      "President Donald Trump -> {'uri': 'http://www.wikidata.org/entity/Q22686', 'text': 'President Donald Trump', 'description': 'president of the United States (2017–2021, 2025–present)', 'types': ['Q5']}\n",
      "Mike Waltz -> {'uri': 'http://www.wikidata.org/entity/Q55386653', 'text': 'Mike Waltz', 'description': 'U.S. National Security Advisor since 2025', 'types': ['Q5']}\n",
      "Ukraine -> {'uri': 'http://www.wikidata.org/entity/Q212', 'text': 'Ukraine', 'description': 'country in Eastern Europe', 'types': ['Q3624078', 'Q6256', 'Q179164', 'Q7270', 'Q619610', 'Q4209223', 'Q4835091']}\n"
     ]
    }
   ],
   "source": [
    "# First we filter by type: the Wikidata type of the entity should correspond\n",
    "# to the named entity label returned by NER. For example, the \"Q5\" type\n",
    "# in Wikidata means \"person\", corresponding to our \"per\" NER annotation.\n",
    "# However, Wikidata can specify multiple types per entity: it is enough\n",
    "# if one of these types match.\n",
    "filtered_wiki_entities = {}\n",
    "for (word, label) in wiki_entities:\n",
    "  entities = wiki_entities[(word, label)]\n",
    "  for entity in entities:\n",
    "    for typ in entity[\"types\"]:\n",
    "      if label not in WIKITYPES or typ in WIKITYPES[label]:\n",
    "        if (word,label) not in filtered_wiki_entities:\n",
    "          filtered_wiki_entities[(word,label)] = []\n",
    "        filtered_wiki_entities[(word,label)].append(entity)\n",
    "        break\n",
    "\n",
    "# Now we choose from the list of possible, type-matching entities.\n",
    "# Many disambiguation methods exist, we will use the crudest one,\n",
    "# which is nevertheless efficient: we select the \"most common\"\n",
    "# interpretation. We approximate this \"commonness\" by taking the\n",
    "# entry that has the lowest Wikidata ID.\n",
    "print(\"Disambiguated entities:\")\n",
    "for (word,label) in filtered_wiki_entities:\n",
    "  min_id = 10000000000000000000000000000000000\n",
    "  for entry in filtered_wiki_entities[(word,label)]:\n",
    "    id = int(entry[\"uri\"].split(\"Q\")[1]) # extract the entity identifier from the URI\n",
    "    if id < min_id:\n",
    "      min_id = id\n",
    "      min_entry = entry\n",
    "  print(word + \" -> \" + str(min_entry))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
